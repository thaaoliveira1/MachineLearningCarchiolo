{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Necessary Dependencies\n",
    "\n",
    "# Data Manipulation\n",
    "import re ## regular expression operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Methods and stopwords text processing\n",
    "import nltk ## natural language toolkit\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "DATASET_COLUMNS=['target','ids','date','flag','user','text']\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "df = pd.read_csv('twitter_data.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "\n",
    "#Exploratory Data Analysis\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data is 1600000\n"
     ]
    }
   ],
   "source": [
    "print('length of data is', len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target     int64\n",
       "ids        int64\n",
       "date      object\n",
       "flag      object\n",
       "user      object\n",
       "text      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for null values\n",
    "np.sum(df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of columns in the data is:   6\n",
      "Count of rows in the data is:   1600000\n"
     ]
    }
   ],
   "source": [
    "#Rows and columns in the dataset\n",
    "print('Count of columns in the data is:  ', len(df.columns))\n",
    "print('Count of rows in the data is:  ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check unique target values\n",
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the number of target values\n",
    "df['target'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Visualization of Target Variables\n",
    "\n",
    "# # Plotting the distribution for dataset.\n",
    "# ax = df.groupby('target').count().plot(kind='bar', title='Distribution of data',legend=False)\n",
    "# ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
    "# # Storing data in lists.\n",
    "# text, sentiment = list(df['text']), list(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.countplot(x='target', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "# data=df[['text','target']]\n",
    "# data['target'] = data['target'].replace(4,1)\n",
    "# data['target'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995    Just woke up. Having no school is the best fee...\n",
       "1599996    TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997    Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998    Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999    happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995    just woke up. having no school is the best fee...\n",
       "1599996    thewdb.com - very cool to hear old walt interv...\n",
       "1599997    are you ready for your mojo makeover? ask me f...\n",
       "1599998    happy 38th birthday to my boo of alll time!!! ...\n",
       "1599999    happy #charitytuesday @thenspcc @sparkscharity...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all text to lowercase\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995    just woke up having no school is the best feel...\n",
       "1599996    thewdbcom  very cool to hear old walt intervie...\n",
       "1599997    are you ready for your mojo makeover ask me fo...\n",
       "1599998    happy 38th birthday to my boo of alll time tup...\n",
       "1599999    happy charitytuesday thenspcc sparkscharity sp...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing punctuations\n",
    "\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "df['text']= df['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995    just woke up having no school is the best feel...\n",
       "1599996    thewdbcom  very cool to hear old walt intervie...\n",
       "1599997    are you ready for your mojo makeover ask me fo...\n",
       "1599998    happy 38th birthday to my boo of alll time tup...\n",
       "1599999    happy charitytuesday thenspcc sparkscharity sp...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing repeating characters\n",
    "\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995    just woke up having no school is the best feel...\n",
       "1599996    thewdbcom  very cool to hear old walt intervie...\n",
       "1599997    are you ready for your mojo makeover ask me fo...\n",
       "1599998    happy 38th birthday to my boo of alll time tup...\n",
       "1599999    happy charitytuesday thenspcc sparkscharity sp...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing URLs\n",
    "\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_URLs(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995    just woke up having no school is the best feel...\n",
       "1599996    thewdbcom  very cool to hear old walt intervie...\n",
       "1599997    are you ready for your mojo makeover ask me fo...\n",
       "1599998    happy th birthday to my boo of alll time tupac...\n",
       "1599999    happy charitytuesday thenspcc sparkscharity sp...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing numeric numbers\n",
    "\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_numbers(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995    just woke up having no school is the best feel...\n",
       "1599996    thewdbcom  very cool to hear old walt intervie...\n",
       "1599997    are you ready for your mojo makeover ask me fo...\n",
       "1599998    happy th birthday to my boo of alll time tupac...\n",
       "1599999    happy charitytuesday thenspcc sparkscharity sp...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing # and @ characters from tweets\n",
    "\n",
    "def cleaning_characters(data):\n",
    "    return re.sub(r'\\@\\w+|\\#','', data)\n",
    "df['text'] = df['text'].apply(lambda x: cleaning_characters(x))\n",
    "df['text'].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Thamires\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Defining set containing all stopwords in English.\n",
    "\n",
    "nltk.download ('stopwords')\n",
    "stop_words = set(stopwords.words( 'english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995                        woke school best feeling ever\n",
       "1599996    thewdbcom cool hear old walt interviews â« ht...\n",
       "1599997                      ready mojo makeover ask details\n",
       "1599998    happy th birthday boo alll time tupac amaru sh...\n",
       "1599999    happy charitytuesday thenspcc sparkscharity sp...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing the above stop words list from the tweet text\n",
    "\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "df['text'] = df['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "df['text'].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995                  [woke, school, best, feeling, ever]\n",
       "1599996    [thewdbcom, cool, hear, old, walt, interviews,...\n",
       "1599997                [ready, mojo, makeover, ask, details]\n",
       "1599998    [happy, th, birthday, boo, alll, time, tupac, ...\n",
       "1599999    [happy, charitytuesday, thenspcc, sparkscharit...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing tokenization of Tweet Text\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['text'] = df['text'].apply(tokenizer.tokenize)\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995                  [woke, school, best, feeling, ever]\n",
       "1599996    [thewdbcom, cool, hear, old, walt, interviews,...\n",
       "1599997                [ready, mojo, makeover, ask, details]\n",
       "1599998    [happy, th, birthday, boo, alll, time, tupac, ...\n",
       "1599999    [happy, charitytuesday, thenspcc, sparkscharit...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming process\n",
    "\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_process(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "df['text']= df['text'].apply(lambda x: stemming_process(x))\n",
    "df['text'].tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995                  [woke, school, best, feeling, ever]\n",
       "1599996    [thewdbcom, cool, hear, old, walt, interviews,...\n",
       "1599997                [ready, mojo, makeover, ask, details]\n",
       "1599998    [happy, th, birthday, boo, alll, time, tupac, ...\n",
       "1599999    [happy, charitytuesday, thenspcc, sparkscharit...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "df['text'] = df['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799999\n",
      "800000\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['target'] != count:\n",
    "#         print(index-1)\n",
    "#         count +=2\n",
    "\n",
    "# where start and where ends negative and positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thamires\\AppData\\Local\\Temp\\ipykernel_16804\\1710829185.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['target'] = df['target'].replace(4,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the target values for easy understanding\n",
    "\n",
    "# df=df[['target', 'text']]\n",
    "# df['target'] = df['target'].replace(4,1)\n",
    "# df['target'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 48.8 MiB for an array with shape (4, 1600000) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thamires\\Documents\\GitHub\\MachineLearningCarchiolo\\preprocessing.ipynb Cell 27\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/preprocessing.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mtail()\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3817\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m   3815\u001b[0m     indexer \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(indexer)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 3817\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_with_is_copy(indexer, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   3819\u001b[0m \u001b[39mif\u001b[39;00m is_single_key:\n\u001b[0;32m   3820\u001b[0m     \u001b[39m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[0;32m   3821\u001b[0m     \u001b[39m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[39m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[39m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n\u001b[0;32m   3825\u001b[0m         \u001b[39m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3903\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3895\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_with_is_copy\u001b[39m(\u001b[39mself\u001b[39m: NDFrameT, indices, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   3896\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3897\u001b[0m \u001b[39m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3898\u001b[0m \u001b[39m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3901\u001b[0m \u001b[39m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3903\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3904\u001b[0m     \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   3905\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_get_axis(axis)\u001b[39m.\u001b[39mequals(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3885\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3874\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take\u001b[39m(\n\u001b[0;32m   3875\u001b[0m     \u001b[39mself\u001b[39m: NDFrameT,\n\u001b[0;32m   3876\u001b[0m     indices,\n\u001b[0;32m   3877\u001b[0m     axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m   3878\u001b[0m     convert_indices: bool_t \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   3879\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   3880\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3881\u001b[0m \u001b[39m    Internal version of the `take` allowing specification of additional args.\u001b[39;00m\n\u001b[0;32m   3882\u001b[0m \n\u001b[0;32m   3883\u001b[0m \u001b[39m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3884\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3885\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_consolidate_inplace()\n\u001b[0;32m   3887\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mtake(\n\u001b[0;32m   3888\u001b[0m         indices,\n\u001b[0;32m   3889\u001b[0m         axis\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   3890\u001b[0m         verify\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   3891\u001b[0m         convert_indices\u001b[39m=\u001b[39mconvert_indices,\n\u001b[0;32m   3892\u001b[0m     )\n\u001b[0;32m   3893\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5985\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5982\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m():\n\u001b[0;32m   5983\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mconsolidate()\n\u001b[1;32m-> 5985\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_protect_consolidate(f)\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5973\u001b[0m, in \u001b[0;36mNDFrame._protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   5971\u001b[0m     \u001b[39mreturn\u001b[39;00m f()\n\u001b[0;32m   5972\u001b[0m blocks_before \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mblocks)\n\u001b[1;32m-> 5973\u001b[0m result \u001b[39m=\u001b[39m f()\n\u001b[0;32m   5974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mblocks) \u001b[39m!=\u001b[39m blocks_before:\n\u001b[0;32m   5975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5983\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace.<locals>.f\u001b[1;34m()\u001b[0m\n\u001b[0;32m   5982\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m():\n\u001b[1;32m-> 5983\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mconsolidate()\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:679\u001b[0m, in \u001b[0;36mBaseBlockManager.consolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    677\u001b[0m bm \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs, verify_integrity\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    678\u001b[0m bm\u001b[39m.\u001b[39m_is_consolidated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 679\u001b[0m bm\u001b[39m.\u001b[39;49m_consolidate_inplace()\n\u001b[0;32m    680\u001b[0m \u001b[39mreturn\u001b[39;00m bm\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1848\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_consolidated():\n\u001b[0;32m   1847\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1848\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m _consolidate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks)\n\u001b[0;32m   1849\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1850\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs \u001b[39m=\u001b[39m _consolidate_with_refs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs)\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2296\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2294\u001b[0m new_blocks: \u001b[39mlist\u001b[39m[Block] \u001b[39m=\u001b[39m []\n\u001b[0;32m   2295\u001b[0m \u001b[39mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[39min\u001b[39;00m grouper:\n\u001b[1;32m-> 2296\u001b[0m     merged_blocks, _ \u001b[39m=\u001b[39m _merge_blocks(\n\u001b[0;32m   2297\u001b[0m         \u001b[39mlist\u001b[39;49m(group_blocks), dtype\u001b[39m=\u001b[39;49mdtype, can_consolidate\u001b[39m=\u001b[39;49m_can_consolidate\n\u001b[0;32m   2298\u001b[0m     )\n\u001b[0;32m   2299\u001b[0m     new_blocks \u001b[39m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2300\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2348\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2341\u001b[0m new_values: ArrayLike\n\u001b[0;32m   2343\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(blocks[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m   2344\u001b[0m     \u001b[39m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[0;32m   2345\u001b[0m     \u001b[39m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[0;32m   2346\u001b[0m     \u001b[39m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m     \u001b[39m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[1;32m-> 2348\u001b[0m     new_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvstack([b\u001b[39m.\u001b[39;49mvalues \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m blocks])  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2349\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2350\u001b[0m     bvals \u001b[39m=\u001b[39m [blk\u001b[39m.\u001b[39mvalues \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m blocks]\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 48.8 MiB for an array with shape (4, 1600000) and data type object"
     ]
    }
   ],
   "source": [
    "# df[['text','target']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating positive and negative tweets\n",
    "\n",
    "# data_pos = df[df['target'] == 1]\n",
    "# data_neg = df[df['target'] == 0]\n",
    "# df = pd.concat([data_pos, data_neg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995   NaN\n",
       "1599996   NaN\n",
       "1599997   NaN\n",
       "1599998   NaN\n",
       "1599999   NaN\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_neg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thamires\\Documents\\GitHub\\MachineLearningCarchiolo\\preprocessing.ipynb Cell 30\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/preprocessing.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Plot words of negative tweets\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/preprocessing.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data_neg \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m800000\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/preprocessing.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m (\u001b[39m20\u001b[39m,\u001b[39m20\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/preprocessing.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m wc \u001b[39m=\u001b[39m WordCloud(max_words \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m , width \u001b[39m=\u001b[39m \u001b[39m1600\u001b[39m , height \u001b[39m=\u001b[39m \u001b[39m800\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/preprocessing.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                collocations\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mgenerate(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m, data_neg)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot words of negative tweets\n",
    "\n",
    "# data_neg = df['text'][:800000]\n",
    "# plt.figure(figsize = (20,20))\n",
    "# wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "#                collocations=False).generate(\" \".join(map(str, data_neg)))\n",
    "# plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tweets of positive tweets\n",
    "\n",
    "# data_pos = data['text'][800000:]\n",
    "# wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "#               collocations=False).generate(\" \".join(data_pos))\n",
    "# plt.figure(figsize = (20,20))\n",
    "# plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599995   NaN\n",
       "1599996   NaN\n",
       "1599997   NaN\n",
       "1599998   NaN\n",
       "1599999   NaN\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # #Separating positive and negative tweets\n",
    "\n",
    "# data_pos = df[df['target'] == 1]\n",
    "# data_neg = df[df['target'] == 0]\n",
    "\n",
    "# data_pos = data_pos.iloc[:int(20000)]\n",
    "# data_neg = data_neg.iloc[:int(20000)]\n",
    "\n",
    "# dataset = pd.concat([data_pos, data_neg])\n",
    "\n",
    "# df['text']=df['text'].str.lower()\n",
    "# df['text'].tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
