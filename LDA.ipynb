{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "DATASET_COLUMNS=['target','ids','date','flag','user','text']\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "df = pd.read_csv('sampled_twitter_data.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "X_test = pd.read_csv('X_test.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "y_train = pd.read_csv('y_train.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "y_test = pd.read_csv('y_test.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.968*\"2\" + 0.030*\"19\" + 0.000*\"21\" + 0.000*\"22\" + 0.000*\"25\" + 0.000*\"24\" + 0.000*\"23\" + 0.000*\"18\" + 0.000*\"17\" + 0.000*\"11\"\n",
      "Topic: 1 \n",
      "Words: 0.991*\"15\" + 0.006*\"0\" + 0.000*\"22\" + 0.000*\"18\" + 0.000*\"17\" + 0.000*\"19\" + 0.000*\"11\" + 0.000*\"20\" + 0.000*\"23\" + 0.000*\"30\"\n",
      "Topic: 2 \n",
      "Words: 0.999*\"7\" + 0.000*\"21\" + 0.000*\"22\" + 0.000*\"25\" + 0.000*\"24\" + 0.000*\"23\" + 0.000*\"30\" + 0.000*\"word\" + 0.000*\"count\" + 0.000*\"8\"\n",
      "Topic: 3 \n",
      "Words: 0.506*\"6\" + 0.494*\"4\" + 0.000*\"21\" + 0.000*\"22\" + 0.000*\"25\" + 0.000*\"23\" + 0.000*\"24\" + 0.000*\"30\" + 0.000*\"word\" + 0.000*\"count\"\n",
      "Topic: 4 \n",
      "Words: 0.990*\"9\" + 0.009*\"20\" + 0.000*\"21\" + 0.000*\"22\" + 0.000*\"25\" + 0.000*\"24\" + 0.000*\"23\" + 0.000*\"30\" + 0.000*\"word\" + 0.000*\"count\"\n",
      "Topic: 5 \n",
      "Words: 0.532*\"12\" + 0.467*\"13\" + 0.000*\"21\" + 0.000*\"22\" + 0.000*\"25\" + 0.000*\"24\" + 0.000*\"23\" + 0.000*\"30\" + 0.000*\"word\" + 0.000*\"count\"\n",
      "Topic: 6 \n",
      "Words: 0.560*\"3\" + 0.440*\"10\" + 0.000*\"21\" + 0.000*\"22\" + 0.000*\"25\" + 0.000*\"24\" + 0.000*\"23\" + 0.000*\"30\" + 0.000*\"word\" + 0.000*\"count\"\n",
      "Topic: 7 \n",
      "Words: 0.908*\"5\" + 0.092*\"14\" + 0.000*\"21\" + 0.000*\"22\" + 0.000*\"25\" + 0.000*\"23\" + 0.000*\"24\" + 0.000*\"30\" + 0.000*\"word\" + 0.000*\"count\"\n",
      "Topic: 8 \n",
      "Words: 0.567*\"8\" + 0.411*\"11\" + 0.021*\"18\" + 0.000*\"21\" + 0.000*\"22\" + 0.000*\"25\" + 0.000*\"24\" + 0.000*\"23\" + 0.000*\"30\" + 0.000*\"word\"\n",
      "Topic: 9 \n",
      "Words: 0.429*\"16\" + 0.335*\"1\" + 0.233*\"17\" + 0.000*\"0\" + 0.000*\"18\" + 0.000*\"19\" + 0.000*\"11\" + 0.000*\"20\" + 0.000*\"23\" + 0.000*\"30\"\n",
      "\n",
      "Coherence Score:  0.8127611313704117\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary and corpus\n",
    "documents = df['text'].tolist()\n",
    "dictionary = corpora.Dictionary([doc.split() for doc in documents])\n",
    "corpus = [dictionary.doc2bow(doc.split()) for doc in documents]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Print topics and their top 10 words\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=[doc.split() for doc in documents], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thamires\\Documents\\GitHub\\MachineLearningCarchiolo\\LDA.ipynb Cell 4\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/LDA.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Train the LDA model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/LDA.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m lda \u001b[39m=\u001b[39m LatentDirichletAllocation(n_components\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/LDA.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m lda\u001b[39m.\u001b[39mfit(df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/LDA.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Visualize the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/LDA.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m pyLDAvis\u001b[39m.\u001b[39menable_notebook()\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:641\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[39m\"\"\"Learn model for the data X with variational Bayes method.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \n\u001b[0;32m    624\u001b[0m \u001b[39mWhen `learning_method` is 'online', use mini-batch update.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 641\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_non_neg_array(\n\u001b[0;32m    642\u001b[0m     X, reset_n_features\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, whom\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLatentDirichletAllocation.fit\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    643\u001b[0m )\n\u001b[0;32m    644\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    645\u001b[0m max_iter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:561\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._check_non_neg_array\u001b[1;34m(self, X, reset_n_features, whom)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[39m\"\"\"check X format\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \n\u001b[0;32m    552\u001b[0m \u001b[39mcheck X format and make sure no negative value in X.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m \n\u001b[0;32m    558\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    559\u001b[0m dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32] \u001b[39mif\u001b[39;00m reset_n_features \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_\u001b[39m.\u001b[39mdtype\n\u001b[1;32m--> 561\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    562\u001b[0m     X,\n\u001b[0;32m    563\u001b[0m     reset\u001b[39m=\u001b[39;49mreset_n_features,\n\u001b[0;32m    564\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    565\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    566\u001b[0m )\n\u001b[0;32m    567\u001b[0m check_non_negative(X, whom)\n\u001b[0;32m    569\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 535\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    536\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    537\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    875\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    876\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    879\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    881\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:2069\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2069\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'ids'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Train the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(df)\n",
    "\n",
    "# Visualize the model\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.sklearn.prepare(lda, df, vectorizer, R=20)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE EXPLANATION\n",
    "\n",
    "This code is using Latent Dirichlet Allocation (LDA), a type of machine learning algorithm that helps to discover topics from a collection of documents. The documents are first preprocessed by splitting the text into individual words and creating a dictionary of all the words that appear in the documents. The corpus is then created by converting the documents into a bag of words format.\n",
    "\n",
    "The LDA model is then trained on the corpus with 10 topics, and the top 10 words for each topic are printed out. These top words represent the most important words that define each topic. The coherence score is also calculated to evaluate how well the topics make sense.\n",
    "\n",
    "Looking at the results, we can see that there are 10 topics represented by numbers from 0 to 9. Each topic has a list of words associated with it, and the number beside each word represents its importance in defining that topic. For example, Topic 0 is defined by the word \"2\" with a weight of 0.968, while Topic 1 is defined by the word \"15\" with a weight of 0.991.\n",
    "\n",
    "The coherence score of 0.81 means that the topics generated by the model are coherent and make sense. This indicates that the LDA model has been successful in discovering meaningful topics from the given set of documents.\n",
    "\n",
    "---\n",
    "\n",
    "The first two lines of code load some text data from a dataframe and create a dictionary and corpus from it. The dictionary is a collection of all the unique words in the text, and the corpus is a collection of documents, where each document is represented as a bag-of-words, i.e., a list of (word_id, word_count) pairs.\n",
    "\n",
    "The next block of code builds an LDA model with 10 topics using the gensim package. LDA stands for Latent Dirichlet Allocation, which is a type of probabilistic topic modeling algorithm used to discover hidden topics from a collection of documents. The LDA model is trained on the corpus using the dictionary, and some parameters like the number of topics, random state, passes, and alpha values are set for the model. The LDA model is then printed with the top 10 words in each topic.\n",
    "\n",
    "The last block of code computes the coherence score for the LDA model. Coherence is a measure of how interpretable the topics are and how well the words in each topic are related. The coherence score is calculated using the CoherenceModel class from gensim, which takes the LDA model, the text data, and the dictionary as input. The coherence score is then printed, which gives an idea of how well the LDA model has performed in discovering topics from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, tuple found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thamires\\Documents\\GitHub\\MachineLearningCarchiolo\\LDA.ipynb Cell 6\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/LDA.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m corpora\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/LDA.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/LDA.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dic\u001b[39m=\u001b[39mgensim\u001b[39m.\u001b[39mcorpora\u001b[39m.\u001b[39mDictionary(corpus)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thamires/Documents/GitHub/MachineLearningCarchiolo/LDA.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m bow_corpus \u001b[39m=\u001b[39m [dic\u001b[39m.\u001b[39mdoc2bow(doc) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m corpus]\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_nnz \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m documents \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_documents(documents, prune_at\u001b[39m=\u001b[39;49mprune_at)\n\u001b[0;32m     79\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreated\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         msg\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbuilt \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_docs\u001b[39m}\u001b[39;00m\u001b[39m documents (total \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pos\u001b[39m}\u001b[39;00m\u001b[39m corpus positions)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    201\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39madding document #\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, docno, \u001b[39mself\u001b[39m)\n\u001b[0;32m    203\u001b[0m     \u001b[39m# update Dictionary with the document\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdoc2bow(document, allow_update\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)  \u001b[39m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[0;32m    206\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mbuilt \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m documents (total \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m corpus positions)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_docs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pos)\n",
      "File \u001b[1;32mc:\\Users\\Thamires\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\corpora\\dictionary.py:246\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    244\u001b[0m counter \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m document:\n\u001b[1;32m--> 246\u001b[0m     counter[w \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(w, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39;49m(w, \u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    248\u001b[0m token2id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken2id\n\u001b[0;32m    249\u001b[0m \u001b[39mif\u001b[39;00m allow_update \u001b[39mor\u001b[39;00m return_missing:\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, tuple found"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "dic=gensim.corpora.Dictionary(corpus)\n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
